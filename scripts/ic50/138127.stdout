Sender: LSF System <lsfadmin@lx08>
Subject: Job 138127: <#BSUB -o %J.stdout;for units in 32;do;    for unit_type in "GraphSAGE";    do;        for n_layers in 3;        do;            layer="$unit_type $units activation tanh ";            architecture=$(for a in `eval echo {1..$n_layers}`; do echo $layer; done);            for inducing_pt in 80;            do;                for annealing in 1.0;                do;                    for regressor in 'vgp';                    do;			for normalize in 0;			do;			    for seed in 0;			    do;			        for pretrain_epoch in -1 20 80 160 280 349;				do;				    for pretrain_frac in 0.1 0.2 0.4 1.0;				    do;                            		    name="${regressor}_${n_layers}_${unit_type}_${units}_${sample_frac}_${annealing}_${inducing_pt}_${normalize}_${seed}";                            		    bsub -q gpuqueue -n 2 -gpu "num=1:j_exclusive=yes" -R "rusage[mem=8] span[hosts=1]" -W 1:30 -o "logs_$name.stdout" -eo "logs_$name.stderr" \;			    		    python3 ic50_supervised.py --data moonshot_pic50 --n_epochs 350 --cuda --regressor_type $regressor --architecture $architecture \;				            --log "${name}.logs" --record_interval 50 --n_inducing_points $inducing_pt --annealing $annealing \;			    		    --normalize $normalize --time_limit "1:00" --filter_outliers --fix_seed --seed $seed --output "/data/chodera/retchinm/pIC50" \;					    --pretrain_epoch $pretrain_epoch --pretrain_frac $pretrain_frac;			    	    done;				done;			    done;	    	        done;                    done;                done;            done;        done;    done;done> in cluster <lila> Done

Job <#BSUB -o %J.stdout;for units in 32;do;    for unit_type in "GraphSAGE";    do;        for n_layers in 3;        do;            layer="$unit_type $units activation tanh ";            architecture=$(for a in `eval echo {1..$n_layers}`; do echo $layer; done);            for inducing_pt in 80;            do;                for annealing in 1.0;                do;                    for regressor in 'vgp';                    do;			for normalize in 0;			do;			    for seed in 0;			    do;			        for pretrain_epoch in -1 20 80 160 280 349;				do;				    for pretrain_frac in 0.1 0.2 0.4 1.0;				    do;                            		    name="${regressor}_${n_layers}_${unit_type}_${units}_${sample_frac}_${annealing}_${inducing_pt}_${normalize}_${seed}";                            		    bsub -q gpuqueue -n 2 -gpu "num=1:j_exclusive=yes" -R "rusage[mem=8] span[hosts=1]" -W 1:30 -o "logs_$name.stdout" -eo "logs_$name.stderr" \;			    		    python3 ic50_supervised.py --data moonshot_pic50 --n_epochs 350 --cuda --regressor_type $regressor --architecture $architecture \;				            --log "${name}.logs" --record_interval 50 --n_inducing_points $inducing_pt --annealing $annealing \;			    		    --normalize $normalize --time_limit "1:00" --filter_outliers --fix_seed --seed $seed --output "/data/chodera/retchinm/pIC50" \;					    --pretrain_epoch $pretrain_epoch --pretrain_frac $pretrain_frac;			    	    done;				done;			    done;	    	        done;                    done;                done;            done;        done;    done;done> was submitted from host <lilac> by user <retchinm> in cluster <lila> at Fri Feb 26 22:02:57 2021
Job was executed on host(s) <lx08>, in queue <cpuqueue>, as user <retchinm> in cluster <lila> at Fri Feb 26 22:02:58 2021
</home/retchinm> was used as the home directory.
</home/retchinm/pinot/scripts/ic50> was used as the working directory.
Started at Fri Feb 26 22:02:58 2021
Terminated at Fri Feb 26 22:03:02 2021
Results reported at Fri Feb 26 22:03:02 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -o %J.stdout
for units in 32
do
    for unit_type in "GraphSAGE"
    do
        for n_layers in 3
        do
            layer="$unit_type $units activation tanh "
            architecture=$(for a in `eval echo {1..$n_layers}`; do echo $layer; done)
            for inducing_pt in 80
            do
                for annealing in 1.0
                do
                    for regressor in 'vgp'
                    do
			for normalize in 0
			do
			    for seed in 0
			    do
			        for pretrain_epoch in -1 20 80 160 280 349
				do
				    for pretrain_frac in 0.1 0.2 0.4 1.0
				    do
                            		    name="${regressor}_${n_layers}_${unit_type}_${units}_${sample_frac}_${annealing}_${inducing_pt}_${normalize}_${seed}"
                            		    bsub -q gpuqueue -n 2 -gpu "num=1:j_exclusive=yes" -R "rusage[mem=8] span[hosts=1]" -W 1:30 -o "logs_$name.stdout" -eo "logs_$name.stderr" \
			    		    python3 ic50_supervised.py --data moonshot_pic50 --n_epochs 350 --cuda --regressor_type $regressor --architecture $architecture \
				            --log "${name}.logs" --record_interval 50 --n_inducing_points $inducing_pt --annealing $annealing \
			    		    --normalize $normalize --time_limit "1:00" --filter_outliers --fix_seed --seed $seed --output "/data/chodera/retchinm/pIC50" \
					    --pretrain_epoch $pretrain_epoch --pretrain_frac $pretrain_frac
			    	    done
				done
			    done
	    	        done
                    done
                done
            done
        done
    done
done

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.85 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     2.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   3 sec.
    Turnaround time :                            5 sec.

The output (if any) follows:

Job <138128> is submitted to queue <gpuqueue>.
Job <138129> is submitted to queue <gpuqueue>.
Job <138130> is submitted to queue <gpuqueue>.
Job <138131> is submitted to queue <gpuqueue>.
Job <138132> is submitted to queue <gpuqueue>.
Job <138133> is submitted to queue <gpuqueue>.
Job <138134> is submitted to queue <gpuqueue>.
Job <138135> is submitted to queue <gpuqueue>.
Job <138136> is submitted to queue <gpuqueue>.
Job <138137> is submitted to queue <gpuqueue>.
Job <138138> is submitted to queue <gpuqueue>.
Job <138139> is submitted to queue <gpuqueue>.
Job <138140> is submitted to queue <gpuqueue>.
Job <138141> is submitted to queue <gpuqueue>.
Job <138142> is submitted to queue <gpuqueue>.
Job <138143> is submitted to queue <gpuqueue>.
Job <138144> is submitted to queue <gpuqueue>.
Job <138145> is submitted to queue <gpuqueue>.
Job <138146> is submitted to queue <gpuqueue>.
Job <138147> is submitted to queue <gpuqueue>.
Job <138148> is submitted to queue <gpuqueue>.
Job <138149> is submitted to queue <gpuqueue>.
Job <138150> is submitted to queue <gpuqueue>.
Job <138151> is submitted to queue <gpuqueue>.
