Sender: LSF System <lsfadmin@lx08>
Subject: Job 14788485: <#BSUB -q cpuqueue;#BSUB -o %J.stdout; for opt in 'adam' 'bbb' 'sgld';do;    for layer in 'GraphConv' 'EdgeConv' 'SAGEConv' 'GINConv' 'SGConv' 'TAGConv' ;    do;        for lr in '1e-5';        do;            name="_gp_"$opt"_"$layer"_"$lr;            bsub -q gpuqueue -J $name -m "ld-gpu ls-gpu lt-gpu lg-gpu lu-gpu" -n 12 -gpu "num=1:j_exclusive=yes" -R "rusage[mem=4] span[hosts=1]" -W 8:00 -o $name/%J.stdout -eo %J.stderr python ../../pinot/app/gp_train.py --layer $layer --optimizer $opt --lr $lr --out $name --n_epochs 1000; done;done;done> in cluster <lila> Done

Job <#BSUB -q cpuqueue;#BSUB -o %J.stdout; for opt in 'adam' 'bbb' 'sgld';do;    for layer in 'GraphConv' 'EdgeConv' 'SAGEConv' 'GINConv' 'SGConv' 'TAGConv' ;    do;        for lr in '1e-5';        do;            name="_gp_"$opt"_"$layer"_"$lr;            bsub -q gpuqueue -J $name -m "ld-gpu ls-gpu lt-gpu lg-gpu lu-gpu" -n 12 -gpu "num=1:j_exclusive=yes" -R "rusage[mem=4] span[hosts=1]" -W 8:00 -o $name/%J.stdout -eo %J.stderr python ../../pinot/app/gp_train.py --layer $layer --optimizer $opt --lr $lr --out $name --n_epochs 1000; done;done;done> was submitted from host <lilac> by user <wangy1> in cluster <lila> at Tue Jun  2 13:08:24 2020
Job was executed on host(s) <lx08>, in queue <cpuqueue>, as user <wangy1> in cluster <lila> at Tue Jun  2 13:08:24 2020
</home/wangy1> was used as the home directory.
</data/chodera/wangyq/pinot/scripts/gp_comparison> was used as the working directory.
Started at Tue Jun  2 13:08:24 2020
Terminated at Tue Jun  2 13:08:25 2020
Results reported at Tue Jun  2 13:08:25 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -q cpuqueue
#BSUB -o %J.stdout

for opt in 'adam' 'bbb' 'sgld'
do
    for layer in 'GraphConv' 'EdgeConv' 'SAGEConv' 'GINConv' 'SGConv' 'TAGConv' 
    do
        for lr in '1e-5'
        do
            name="_gp_"$opt"_"$layer"_"$lr
            bsub -q gpuqueue -J $name -m "ld-gpu ls-gpu lt-gpu lg-gpu lu-gpu" -n 12 -gpu "num=1:j_exclusive=yes" -R "rusage[mem=4] span[hosts=1]" -W 8:00 -o $name/%J.stdout -eo %J.stderr python ../../pinot/app/gp_train.py --layer $layer --optimizer $opt --lr $lr --out $name --n_epochs 1000

done
done
done


------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.62 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     2.00 GB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   2 sec.
    Turnaround time :                            1 sec.

The output (if any) follows:

Job <14788486> is submitted to queue <gpuqueue>.
Job <14788487> is submitted to queue <gpuqueue>.
Job <14788488> is submitted to queue <gpuqueue>.
Job <14788489> is submitted to queue <gpuqueue>.
Job <14788490> is submitted to queue <gpuqueue>.
Job <14788491> is submitted to queue <gpuqueue>.
Job <14788492> is submitted to queue <gpuqueue>.
Job <14788493> is submitted to queue <gpuqueue>.
Job <14788494> is submitted to queue <gpuqueue>.
Job <14788495> is submitted to queue <gpuqueue>.
Job <14788496> is submitted to queue <gpuqueue>.
Job <14788497> is submitted to queue <gpuqueue>.
Job <14788498> is submitted to queue <gpuqueue>.
Job <14788499> is submitted to queue <gpuqueue>.
Job <14788500> is submitted to queue <gpuqueue>.
Job <14788501> is submitted to queue <gpuqueue>.
Job <14788502> is submitted to queue <gpuqueue>.
Job <14788503> is submitted to queue <gpuqueue>.
