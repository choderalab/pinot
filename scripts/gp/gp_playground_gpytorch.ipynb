{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import pinot\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duc/anaconda3/envs/torch-research/lib/python3.7/site-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "data = pinot.data.esol()\n",
    "ds_tr, ds_te = pinot.data.utils.split(data, [4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "ds_tr_onebatch = pinot.data.utils.batch(ds_tr, len(ds_tr))\n",
    "ds_tr_batched  = pinot.data.utils.batch(ds_tr, 32)\n",
    "ds_te_onebatch = pinot.data.utils.batch(ds_te, len(ds_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, yhat):\n",
    "    return torch.sqrt(torch.mean((y.flatten()-yhat.flatten())**2))\n",
    "\n",
    "def run_experiment(num_inducing, kernel_name=None, kernel_params=None, mean_name=\"LinearMean\", beta=1.0, device=\"cpu\", n_epochs=50):\n",
    "    dev = torch.device(device)\n",
    "    net_variational_gp = pinot.Net(\n",
    "        pinot.representation.Sequential(\n",
    "            pinot.representation.dgl_legacy.gn(kwargs={\"allow_zero_in_degree\":True}),\n",
    "                [64, 'relu', 64, 'relu', 64, 'relu']),\n",
    "        output_regressor_class=pinot.regressors.VariationalGP,\n",
    "        num_inducing_points=num_inducing,\n",
    "        num_data=902,\n",
    "        beta = beta,\n",
    "        covar = getattr(gpytorch.kernels, kernel_name)(**kernel_params) if kernel_name is not None else None\n",
    "    ).to(dev)\n",
    "    \n",
    "    lr = 1e-4\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': net_variational_gp.representation.parameters(), 'weight_decay': 1e-4},\n",
    "        {'params': net_variational_gp.output_regressor.parameters(), 'lr': lr*0.1}\n",
    "    ], lr=lr)\n",
    "\n",
    "    for n in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        for (g, y) in ds_tr_onebatch:\n",
    "            optimizer.zero_grad()\n",
    "            loss = net_variational_gp.loss(g.to(dev), y.flatten().to(dev))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    train_rmse = rmse(net_variational_gp.condition(ds_tr_onebatch[0][0].to(dev)).mean.cpu(), ds_tr_onebatch[0][1])\n",
    "    test_rmse = rmse(net_variational_gp.condition(ds_te_onebatch[0][0].to(dev)).mean.cpu(), ds_te_onebatch[0][1])\n",
    "    del net_variational_gp\n",
    "    print(f\"Train rmse = {train_rmse}, test rmse = {test_rmse} for {kernel_name} kernel, beta = {beta}, num_inducing_points = {num_inducing}, mean = {mean_name}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rmse = 0.8605661392211914, test rmse = 0.7832201719284058 for None kernel, beta = 1.0, num_inducing_points = 200, mean = LinearMean\n",
      "Train rmse = 0.7850788831710815, test rmse = 0.7508814334869385 for None kernel, beta = 1.0, num_inducing_points = 400, mean = LinearMean\n",
      "Train rmse = 0.8468971848487854, test rmse = 0.8037276268005371 for None kernel, beta = 1.0, num_inducing_points = 600, mean = LinearMean\n"
     ]
    }
   ],
   "source": [
    "for (kernel_name, kernel_params) in [\n",
    "        (None, None),\n",
    "        (\"PolynomialKernel\", {\"power\":3}),\n",
    "        (\"RBFKernel\", {})\n",
    "    ]:\n",
    "    \n",
    "    for num_inducing in [200, 400, 600]:\n",
    "        run_experiment(num_inducing, kernel_name, kernel_params, device=\"cuda\", n_epochs=2000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
