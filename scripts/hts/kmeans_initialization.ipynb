{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing k-means initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--initialize_k_means'], dest='initialize_k_means', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='Sets inducing points with k-means if equal to 1', metavar=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running functions\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\"HTS supervised learning\")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--regressor_type', \n",
    "    type=str,\n",
    "    default='gp',\n",
    "    choices=[\"gp\", \"nn\", \"vgp\"],\n",
    "    help=\"Type of output regressor, Gaussian Process, Variational GP or Neural Networks\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    type=float,\n",
    "    default=1e-4,\n",
    "    help=\"learning rate of optimizer\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--optimizer',\n",
    "    type=str,\n",
    "    default='Adam',\n",
    "    help=\"Optimization algorithm\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--data',\n",
    "    type=str,\n",
    "    default=\"mpro_hts\",\n",
    "    help=\"Labeled data set name\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--n_epochs',\n",
    "    type=int,\n",
    "    default=500,\n",
    "    help=\"number of training epochs\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--architecture',\n",
    "    nargs=\"+\",\n",
    "    type=str,\n",
    "    default=[32, \"tanh\", 32, \"tanh\", 32, \"tanh\"],\n",
    "    help=\"Graph neural network architecture\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--cuda',\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Using GPU\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--output',\n",
    "    type=str,\n",
    "    default=\"out\",\n",
    "    help=\"Name of folder to store results\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--time_limit',\n",
    "    type=str,\n",
    "    default=\"200:00\",\n",
    "    help=\"Limit on training time. Format is hour:minute.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--log',\n",
    "    type=str,\n",
    "    default=\"logs\",\n",
    "    help=\"Log file\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--weight_decay',\n",
    "    default=0.01,\n",
    "    type=float,\n",
    "    help=\"Weight decay for optimizer\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    default=32,\n",
    "    type=int,\n",
    "    help=\"Batch size\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--sample_frac',\n",
    "    nargs=\"+\",\n",
    "    type=float,\n",
    "    default=0.005, # 0.1\n",
    "    help=\"Proportion of dataset to use\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--label_split',\n",
    "    nargs=\"+\",\n",
    "    type=list,\n",
    "    default=[4, 1],\n",
    "    help=\"Training-testing split for labeled data\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--index',\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Arbitrary index to append to logs\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--annealing',\n",
    "    type=float,\n",
    "    default=1.0,\n",
    "    help=\"Scaling factor on the KL term in the variational inference loss\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--n_inducing_points',\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help=\"Number of inducing points to use for variational inference\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--record_interval',\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help=\"Number of intervals before recording metrics\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--normalize',\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"Number of inducing points to use for variational inference\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--fix_seed',\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Whether to fix random seed\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--filter_outliers',\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Whether to filter huge outliers.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--filter_neg_train',\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Whether to filter negatives in the training set.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--filter_neg_test',\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Whether to filter negatives in the testing set.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"Setting the seed for random sampling\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--filter_threshold',\n",
    "    type=float,\n",
    "    default=-2.0, # 0.1\n",
    "    help=\"Proportion of dataset to use\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--mu_mean',\n",
    "    type=float,\n",
    "    default=0.0,\n",
    "    help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--mu_std',\n",
    "    type=float,\n",
    "    default=0.1,\n",
    "    help=\"Epoch of training curve for pretrained representation; -1 means no pretraining\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--std_value',\n",
    "    type=float,\n",
    "    default=-2,\n",
    "    help=\"Epoch of training curve for pretrained representation; -1 means no pretraining\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--initialize_k_means',\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Sets inducing points with k-means if equal to 1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vars(parser.parse_args([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only default values\n",
    "args = vars(parser.parse_args([]))\n",
    "args['regressor_type'] = 'vgp'\n",
    "args['sample_frac'] = 0.1\n",
    "args['n_epochs'] = 350\n",
    "args['filter_neg_train'] = True\n",
    "args['filter_neg_test'] = True\n",
    "\n",
    "# make dotdict\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "args = dotdict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify accelerator (if any)\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu:0\")\n",
    "\n",
    "layer_type = args.architecture[0]\n",
    "n_layers = len(args.architecture) // 4\n",
    "n_units = args.architecture[1]\n",
    "activation = args.architecture[3]\n",
    "\n",
    "seed = args.seed if args.fix_seed else None\n",
    "savefile = (f'reg={args.regressor_type}_a={n_layers}x_{n_units}x'\n",
    "            f'_{layer_type}_{activation}_n={args.n_epochs}_b={args.batch_size}'\n",
    "            f'_wd={args.weight_decay}_lsp={args.label_split[0]}_frac={args.sample_frac}'\n",
    "            f'_anneal={args.annealing}_induce={args.n_inducing_points}_normalize={args.normalize}'\n",
    "            f'_{args.index}_seed={seed}_filterthreshold={args.filter_threshold}'\n",
    "            f'_mumean={args.mu_mean}_mustd={args.mu_std}_stdvalue={args.std_value}'\n",
    "            f'_filter_neg_train={args.filter_neg_train}_filter_neg_test={args.filter_neg_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\miniconda3\\envs\\pinot\\lib\\site-packages\\dgl\\base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# otherwise, load from scratch\n",
    "data = getattr(pinot.data, args.data)(sample_frac=args.sample_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to cuda\n",
    "data = data.to(device)\n",
    "\n",
    "# filter out huge outliers\n",
    "if args.filter_outliers:\n",
    "    data.ds = list(filter(lambda x: x[1] > args.filter_threshold, data))\n",
    "\n",
    "# Split the labeled moonshot data into training set and test set\n",
    "train_data, test_data = data.split(args.label_split, seed=seed)\n",
    "\n",
    "if args.filter_neg_train:\n",
    "    train_data.ds = list(filter(lambda x: x[1] > 0.0, train_data))\n",
    "\n",
    "if args.filter_neg_test:\n",
    "    test_data.ds = list(filter(lambda x: x[1] > 0.0, test_data))\n",
    "\n",
    "\n",
    "# Normalize training data using train mean and train std\n",
    "if args.normalize:\n",
    "    gs, ys_tr = zip(*train_data.ds)\n",
    "    ys_tr = torch.cat(ys_tr).reshape(-1, 1)\n",
    "    mean_tr, std_tr = ys_tr.mean(), ys_tr.std()\n",
    "    ys_norm_tr = (ys_tr - mean_tr)/std_tr\n",
    "    train_data.ds = list(zip(gs, ys_norm_tr))\n",
    "\n",
    "    # Normalize testing data using train mean and train std\n",
    "    gs, ys_te = zip(*test_data.ds)\n",
    "    ys_te = torch.cat(ys_te).reshape(-1, 1)\n",
    "    ys_norm_te = (ys_te - mean_tr)/std_tr\n",
    "    test_data.ds = list(zip(gs, ys_norm_te))\n",
    "\n",
    "\n",
    "# Set batch size and log\n",
    "batch_size = args.batch_size if args.regressor_type != 'gp' else len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net_and_optimizer(args):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    representation = pinot.representation.sequential.SequentialMix(\n",
    "        args.architecture,\n",
    "    )\n",
    "\n",
    "    if args.regressor_type == \"gp\":\n",
    "        output_regressor = pinot.regressors.ExactGaussianProcessRegressor\n",
    "    elif args.regressor_type == \"nn\":\n",
    "        output_regressor = pinot.regressors.NeuralNetworkRegressor \n",
    "    else:\n",
    "        output_regressor = pinot.regressors.VariationalGaussianProcessRegressor\n",
    "\n",
    "    # First train a fully supervised Net to use as Baseline\n",
    "    net = pinot.Net(\n",
    "        representation=representation,\n",
    "        output_regressor_class=output_regressor,\n",
    "        n_inducing_points=args.n_inducing_points\n",
    "    )\n",
    "    optimizer = pinot.app.utils.optimizer_translation(\n",
    "        opt_string=args.optimizer,\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "    net.to(device)\n",
    "    return net, optimizer(net)\n",
    "\n",
    "sup_net, optimizer = get_net_and_optimizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7116, -0.8450, -0.9564,  ...,  0.7036,  0.7349, -0.9047],\n",
       "        [ 0.6489, -0.9565,  0.3722,  ..., -0.9015,  0.2913,  0.7152],\n",
       "        [ 0.3899, -0.9063,  0.5909,  ...,  0.6339,  0.6003,  0.1481],\n",
       "        ...,\n",
       "        [-0.5034,  0.6223, -0.7590,  ..., -0.3028,  0.7848, -0.8143],\n",
       "        [ 0.2797, -0.1403, -0.8969,  ..., -0.1860, -0.8857, -0.3679],\n",
       "        [-0.0528, -0.6206, -0.4435,  ...,  0.4205, -0.3902,  0.0913]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inducing points!\n",
    "sup_net.output_regressor.x_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initial_values_for_GP(train_dataset, feature_extractor, n_inducing_points):\n",
    "    \"\"\" Assumes that both dataset and feature extractor\n",
    "        are either cuda or not cuda.\n",
    "        Also assumes the train_dataset is unbatched\n",
    "    \"\"\"\n",
    "    steps = 10\n",
    "    indices = torch.randperm(len(train_dataset))[:1000].chunk(steps)\n",
    "    f_X_samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(steps):\n",
    "            f_X_sample = torch.cat([\n",
    "                feature_extractor(train_dataset[j.item()][0])\n",
    "                for j in indices[i]\n",
    "            ])\n",
    "            f_X_samples.append(f_X_sample)\n",
    "\n",
    "    return torch.cat(f_X_samples)\n",
    "\n",
    "def _get_kmeans(f_X_sample, n_inducing_points):\n",
    "    \"\"\" Get k means for multidimensional input.\n",
    "    \"\"\"\n",
    "    kmeans = cluster.MiniBatchKMeans(\n",
    "        n_clusters=n_inducing_points, batch_size=n_inducing_points * 10\n",
    "    )\n",
    "    kmeans.fit(f_X_sample.cpu().numpy())\n",
    "    cluster_centers = torch.from_numpy(kmeans.cluster_centers_)\n",
    "\n",
    "    return cluster_centers\n",
    "\n",
    "def initialize_inducing_points(train_dataset, feature_extractor, n_inducing_points):\n",
    "    \"\"\" Get initial inducing points for variational GP model.\n",
    "    \"\"\"\n",
    "    f_X_sample = _initial_values_for_GP(\n",
    "        train_dataset,\n",
    "        feature_extractor,\n",
    "        n_inducing_points\n",
    "    )\n",
    "\n",
    "    initial_inducing_points = _get_kmeans(f_X_sample, n_inducing_points)\n",
    "    return initial_inducing_points "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set inducing points using k-means of output from regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\miniconda3\\envs\\pinot\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:887: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2048 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "init_induce_points = pinot.app.utils.initialize_inducing_points(\n",
    "    train_dataset = train_data,\n",
    "    feature_extractor = sup_net.representation,\n",
    "    n_inducing_points = sup_net.output_regressor.n_inducing_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7.1233, -19.1180, -11.8646,  ...,   8.5274,   1.2081,  -6.4313],\n",
       "        [  9.8710, -24.8199, -16.1313,  ...,  10.8500,   4.6070,  -7.0693],\n",
       "        [  5.0331, -13.6795,  -9.0649,  ...,   6.7591,   1.2434,  -4.4143],\n",
       "        ...,\n",
       "        [  7.8042, -19.8476, -12.8215,  ...,   8.7871,   3.2169,  -5.8018],\n",
       "        [  9.4224, -24.2821, -16.0787,  ...,  10.1795,   5.5139,  -7.4783],\n",
       "        [  7.1576, -18.9278, -13.0275,  ...,   9.6793,   2.7891,  -5.5595]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_induce_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  8.6639, -23.6054, -15.4143,  ...,  11.5641,   2.2843,  -7.3783],\n",
       "        [  5.6081, -15.6754, -10.3079,  ...,   8.1157,   0.4025,  -5.3210],\n",
       "        [  7.8686, -20.1883, -12.5105,  ...,   8.4455,   2.7434,  -5.9046],\n",
       "        ...,\n",
       "        [  6.9526, -18.0050, -11.6376,  ...,   7.3911,   3.5004,  -5.6421],\n",
       "        [ 11.4527, -27.4108, -18.0672,  ...,  10.5639,   6.9197,  -8.0093],\n",
       "        [  7.0987, -19.2762, -12.9772,  ...,  10.0077,   1.7761,  -5.9514]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_induce_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\miniconda3\\envs\\pinot\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:887: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 2048 or by setting the environment variable OMP_NUM_THREADS=4\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-b27521142aa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# mini-batch if we're using variational GP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# if the net regressor is a VGP\n",
    "# set inducing points using k-means\n",
    "# before mini-batching\n",
    "if isinstance(\n",
    "    sup_net.output_regressor,\n",
    "    pinot.regressors.VariationalGaussianProcessRegressor\n",
    ") and args.initialize_k_means:\n",
    "\n",
    "    init_induce_points = pinot.app.utils.initialize_inducing_points(\n",
    "        train_dataset = train_data,\n",
    "        feature_extractor = sup_net.representation,\n",
    "        n_inducing_points = sup_net.output_regressor.n_inducing_points\n",
    "    )\n",
    "\n",
    "    sup_net.output_regressor.x_tr = torch.nn.Parameter(\n",
    "        init_induce_points\n",
    "    )\n",
    "    sup_net.to(device)\n",
    "\n",
    "\n",
    "# mini-batch if we're using variational GP\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# get results\n",
    "metrics = [\n",
    "    pinot.pearsonr,\n",
    "    pinot.absolute_error,\n",
    "    pinot.y_hat,\n",
    "    pinot.rmse,\n",
    "    pinot.r2,\n",
    "    pinot.avg_nll\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10\n",
    "indices = torch.randperm(len(train_dataset))[:1000].chunk(steps)\n",
    "f_X_samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(steps):\n",
    "        f_X_sample = torch.cat([\n",
    "            feature_extractor(train_dataset[j.item()][0])\n",
    "            for j in indices[i]\n",
    "        ])\n",
    "        f_X_samples.append(f_X_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
