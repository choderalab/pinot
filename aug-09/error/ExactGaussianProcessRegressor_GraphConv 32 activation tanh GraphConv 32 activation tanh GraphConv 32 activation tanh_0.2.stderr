Using backend: pytorch
Traceback (most recent call last):
  File "scripts/semi_supervised/learning_curve_semi.py", line 299, in <module>
    train_and_test_semi_supervised(net, optimizer, train_semi, train_labeled, test_labeled, args.n_epochs,
  File "scripts/semi_supervised/learning_curve_semi.py", line 244, in train_and_test_semi_supervised
    train.train()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 117, in train
    epoch_loss = self.train_once()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 92, in train_once
    loss = torch.sum(self.net.loss(*x))
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 155, in loss
    total_loss = self.loss_unsupervised(g, h) * self.unsup_scale
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 267, in loss_unsupervised
    recon_loss = self.decoder.decode_and_compute_recon_error(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 295, in decode_and_compute_recon_error
    decoded_subgraphs = self.forward(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 412, in forward
    decoded_subgraphs = [
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 413, in <listcomp>
    self.decode(g_sample.ndata["h"]) for g_sample in gs_unbatched
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 372, in decode
    temp2 = z.repeat(n, 1)  # Shape is also (n, n, Dx2)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.80 GiB total capacity; 6.87 GiB already allocated; 3.12 MiB free; 6.90 GiB reserved in total by PyTorch)
Using backend: pytorch
Traceback (most recent call last):
  File "scripts/semi_supervised/learning_curve_semi.py", line 299, in <module>
    train_and_test_semi_supervised(net, optimizer, train_semi, train_labeled, test_labeled, args.n_epochs,
  File "scripts/semi_supervised/learning_curve_semi.py", line 244, in train_and_test_semi_supervised
    train.train()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 117, in train
    epoch_loss = self.train_once()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 92, in train_once
    loss = torch.sum(self.net.loss(*x))
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 155, in loss
    total_loss = self.loss_unsupervised(g, h) * self.unsup_scale
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 267, in loss_unsupervised
    recon_loss = self.decoder.decode_and_compute_recon_error(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 295, in decode_and_compute_recon_error
    decoded_subgraphs = self.forward(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 412, in forward
    decoded_subgraphs = [
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 413, in <listcomp>
    self.decode(g_sample.ndata["h"]) for g_sample in gs_unbatched
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 373, in decode
    temp = torch.cat(
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.80 GiB total capacity; 6.88 GiB already allocated; 3.12 MiB free; 6.90 GiB reserved in total by PyTorch)
Using backend: pytorch
Traceback (most recent call last):
  File "scripts/semi_supervised/learning_curve_semi.py", line 299, in <module>
    train_and_test_semi_supervised(net, optimizer, train_semi, train_labeled, test_labeled, args.n_epochs,
  File "scripts/semi_supervised/learning_curve_semi.py", line 244, in train_and_test_semi_supervised
    train.train()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 117, in train
    epoch_loss = self.train_once()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 92, in train_once
    loss = torch.sum(self.net.loss(*x))
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 155, in loss
    total_loss = self.loss_unsupervised(g, h) * self.unsup_scale
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 267, in loss_unsupervised
    recon_loss = self.decoder.decode_and_compute_recon_error(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 295, in decode_and_compute_recon_error
    decoded_subgraphs = self.forward(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 412, in forward
    decoded_subgraphs = [
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 413, in <listcomp>
    self.decode(g_sample.ndata["h"]) for g_sample in gs_unbatched
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 373, in decode
    temp = torch.cat(
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 10.23 GiB already allocated; 3.06 MiB free; 10.26 GiB reserved in total by PyTorch)
Using backend: pytorch
Traceback (most recent call last):
  File "scripts/semi_supervised/learning_curve_semi.py", line 299, in <module>
    train_and_test_semi_supervised(net, optimizer, train_semi, train_labeled, test_labeled, args.n_epochs,
  File "scripts/semi_supervised/learning_curve_semi.py", line 244, in train_and_test_semi_supervised
    train.train()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 117, in train
    epoch_loss = self.train_once()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 92, in train_once
    loss = torch.sum(self.net.loss(*x))
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 155, in loss
    total_loss = self.loss_unsupervised(g, h) * self.unsup_scale
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 267, in loss_unsupervised
    recon_loss = self.decoder.decode_and_compute_recon_error(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 295, in decode_and_compute_recon_error
    decoded_subgraphs = self.forward(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 412, in forward
    decoded_subgraphs = [
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 413, in <listcomp>
    self.decode(g_sample.ndata["h"]) for g_sample in gs_unbatched
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 373, in decode
    temp = torch.cat(
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.80 GiB total capacity; 6.88 GiB already allocated; 3.12 MiB free; 6.90 GiB reserved in total by PyTorch)
Using backend: pytorch
Traceback (most recent call last):
  File "scripts/semi_supervised/learning_curve_semi.py", line 299, in <module>
    train_and_test_semi_supervised(net, optimizer, train_semi, train_labeled, test_labeled, args.n_epochs,
  File "scripts/semi_supervised/learning_curve_semi.py", line 244, in train_and_test_semi_supervised
    train.train()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 117, in train
    epoch_loss = self.train_once()
  File "/home/nguyenm5/coding/pinot/pinot/app/experiment.py", line 92, in train_once
    loss = torch.sum(self.net.loss(*x))
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 155, in loss
    total_loss = self.loss_unsupervised(g, h) * self.unsup_scale
  File "/home/nguyenm5/coding/pinot/pinot/generative/semi_supervised_net.py", line 267, in loss_unsupervised
    recon_loss = self.decoder.decode_and_compute_recon_error(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 295, in decode_and_compute_recon_error
    decoded_subgraphs = self.forward(g, z_sample)
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 412, in forward
    decoded_subgraphs = [
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 413, in <listcomp>
    self.decode(g_sample.ndata["h"]) for g_sample in gs_unbatched
  File "/home/nguyenm5/coding/pinot/pinot/generative/decoder.py", line 382, in decode
    E_tilde = self.e_tensor_to_E_tilde(e_tensor.view(n * n, 2 * h))
  File "/home/nguyenm5/anaconda3/envs/pinot/lib/python3.8/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/nguyenm5/anaconda3/envs/pinot/lib/python3.8/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/home/nguyenm5/anaconda3/envs/pinot/lib/python3.8/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/nguyenm5/anaconda3/envs/pinot/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 94, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/nguyenm5/anaconda3/envs/pinot/lib/python3.8/site-packages/torch/nn/functional.py", line 1063, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 10.24 GiB already allocated; 3.06 MiB free; 10.26 GiB reserved in total by PyTorch)
Using backend: pytorch
/home/nguyenm5/anaconda3/envs/pinot/lib/python3.8/site-packages/dgl/base.py:25: UserWarning: Currently adjacency_matrix() returns a matrix with destination as rows by default.  In 0.5 the result will have source as rows (i.e. transpose=True)
  warnings.warn(msg, warn_type)
/home/nguyenm5/coding/pinot/pinot/metrics.py:45: UserWarning: Using a target size (torch.Size([18, 1])) that is different to the input size (torch.Size([18])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return torch.nn.functional.mse_loss(y, y_hat).pow(0.5)
/home/nguyenm5/coding/pinot/pinot/metrics.py:45: UserWarning: Using a target size (torch.Size([355, 1])) that is different to the input size (torch.Size([355])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return torch.nn.functional.mse_loss(y, y_hat).pow(0.5)
Using backend: pytorch
/home/nguyenm5/anaconda3/envs/pinot/lib/python3.8/site-packages/dgl/base.py:25: UserWarning: Currently adjacency_matrix() returns a matrix with destination as rows by default.  In 0.5 the result will have source as rows (i.e. transpose=True)
  warnings.warn(msg, warn_type)
/home/nguyenm5/coding/pinot/pinot/metrics.py:45: UserWarning: Using a target size (torch.Size([37, 1])) that is different to the input size (torch.Size([37])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return torch.nn.functional.mse_loss(y, y_hat).pow(0.5)
/home/nguyenm5/coding/pinot/pinot/metrics.py:45: UserWarning: Using a target size (torch.Size([336, 1])) that is different to the input size (torch.Size([336])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return torch.nn.functional.mse_loss(y, y_hat).pow(0.5)
